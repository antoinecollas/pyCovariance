import numpy as np
import scipy as sp
import warnings

from .covariance_clustering_functions import Riemannian_distance_covariance, Riemannian_mean_covariance
from .generic_functions import *

def compute_feature_Covariance_texture(, args):
    """ Serve to compute feature for Covariance and texture classificaiton.
        We use vech opeartion to save memory space on covariance.
        ----------------------------------------------------------------------
        Inputs:
        --------
            *  = a (p, N) array where p is the dimension of data and N the number
                    of samples used for estimation
            * args = (系, iter_max) for Tyler estimator, where
                ** 系 = tolerance for convergence
                ** iter_max = number of iterations max

        Outputs:
        ---------
            *  = the feature for classification
        """

    系, iter_max = args
    , 未, iteration = tyler_estimator_covariance_normalisedet(np.squeeze(), 系, iter_max)
     = (+.conj().T)/2
     = np.diagonal(np.squeeze().conj().T@np.linalg.inv()@np.squeeze())
    return list( np.hstack([vech(), ]) )


def compute_feature_Covariance_texture_mean(, args):
    """ Serve to compute feature for Covariance and texture classificaiton.
        We use vech opeartion to save memory space on covariance. Texture is
        computed as the mean over the window.
        ----------------------------------------------------------------------
        Inputs:
        --------
            *  = a (p, N) array where p is the dimension of data and N the number
                    of samples used for estimation
            * args = (系, iter_max) for Tyler estimator, where
                ** 系 = tolerance for convergence
                ** iter_max = number of iterations max

        Outputs:
        ---------
            *  = the feature for classification
        """

    系, iter_max = args
    , 未, iteration = tyler_estimator_covariance_normalisedet(np.squeeze(), 系, iter_max)
     = np.diagonal(np.squeeze().conj().T@np.linalg.inv()@np.squeeze())
    return list( np.hstack([vech(), np.mean()]) )


def Riemannian_distance_covariance_texture(_1, _2, params=None):
    """ Riemannian distance on covariance + texture parameters
        ----------------------------------------------------------------------
        Inputs:
        --------
            * _1 = a (p*(p+1)/2+N,) numpy array corresponding to the stack of vech 
                    of the covariance matrix and textures for sample 1
            * _2 = a (p*(p+1)/2+N,) numpy array corresponding to the stack of vech 
                    of the covariance matrix and textures for sample 2
            * params = (p, N)
        Outputs:
        ---------
            * d = the distance between samples
        """
    p, N = params
    
    dist_cov = Riemannian_distance_covariance(_1[:int(p*(p+1)/2)],_2[:int(p*(p+1)/2)])
    
    _1 = _1[int(p*(p+1)/2):]
    _2 = _2[int(p*(p+1)/2):]
    dist_ = np.linalg.norm(np.log(_1)-np.log(_2))

    d = np.sqrt((1/p)*(dist_cov**2)+(1/n)*(dist_**2))

    return np.real(d)


def Riemannian_mean_covariance_texture(_class, mean_parameters=None):
    """ Riemannian mean on covariance + texture manifold:
        ----------------------------------------------------------------------
        Inputs:
        --------
            * _class = array of shape (p*(p+1)/2 + N, M) corresponding to 
                        samples in class
            * mean_parameters = (系, 系_step, tol, iter_max, enable_multi, number_of_threads) where
                * 系_start controls the speed of the gradient descent at first step
                * 系_update is the step of in line search: 系 = 系_start * 系_update at each descending step
                * tol is the tolerance to stop the gradient descent
                * iter_max is the maximum number of iteration
                * enable_multi is a boolean to activate parrallel computation
                * number_of_threads is the number of threas for parrallel computation

        Outputs:
        ---------
            *  = the vech of Riemannian mean
        """

    p, N, 系, 系_step, tol, iter_max, enable_multi, number_of_threads = mean_parameters

    # Splitting covariance and texture features
    _ = _class[:int(p*(p+1)/2),:]
    _ = _class[int(p*(p+1)/2):,:]

    # Compuitng Riemannian mean on PDH set
    _mean = Riemannian_mean_covariance(_, (系, 系_step, tol, iter_max, enable_multi, number_of_threads))

    # Computing geometric mean for textures
    _mean = np.exp( np.sum(np.log(_), axis=1) * (1.0/_.shape[1]) )

    # Staking and returning results
    return np.hstack([_mean, _mean])


def Riemannian_distance_covariance_texture_old(_1, _2, params=None):
    """ (old  but may be useful) Riemannian distance on covariance + 
        texture parameter
        ----------------------------------------------------------------------
        Inputs:
        --------
            * _1 = a (p,) numpy array corresponding to the stack of vech 
                    of the covariance matrix and texture for sample 1
            * _2 = a (p,) numpy array corresponding to the stack of vech 
                    of the covariance matrix and texture for sample 2
            * params = (伪, 尾, tol, iter_max, scale) where
                    ** 伪 = scale for independent terms
                    ** 尾 = scale for crossed-terms
                    ** 系 = tolerance for convergence
                    ** iter_max = number of iterations max
                    ** scale = either 'log' or 'linear' for distance scale
        Outputs:
        ---------
            * d = the distance between samples
        """
    伪, 尾, tol, iter_max, scale = params
    _1 = unvech(_1[:-1])
    _2 = unvech(_2[:-1])
    _1 = _1[-1]
    _2 = _2[-1]
    i_1_sqm = np.linalg.inv(sp.linalg.sqrtm(_1))
    d = 伪 * np.linalg.norm( np.log( i_1_sqm @ _2 @ i_1_sqm ) ) + \
                伪 * np.sum( np.log( _1 / _2 )**2 ) + \
                尾 * np.sum( np.log( _1 / _2 ) )**2

    if scale=='log':
        return np.real(d)
    else:
        return np.exp(np.real(d))

