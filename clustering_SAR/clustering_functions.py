from multiprocessing import Process, Queue
import numpy as np
import scipy as sp
from tqdm import tqdm
import time
import warnings

from .generic_functions import *

# ----------------------------------------------------------------------------
# 1) K-means algorithm in a general multivariate context with an arbitary 
#    distance and an arbitrary way to chose clusters center
# ----------------------------------------------------------------------------
def compute_distance_k_means(X, mu, distance, enable_multi=False, queue=None):
    # -----------------------------------------------------------
    # Definition of function to be executed in parallel (or not)
    # -----------------------------------------------------------
    (p, N) = X.shape
    (p, K) = mu.shape
    d = np.empty((N, K))  # To store distance from each class
    for n in tqdm(range(N)):  # Looping on all samples
        for k in range(K):  # Looping on al classes
            d[n, k] = distance(mu[:, k], X[:, n])
    if enable_multi:
        queue.put(ğ)
    else:
        return ğ


def compute_all_distances_k_means(
    ğ—,
    ğ›,
    distance,
    enable_multi=False,
    number_of_threads=4
):
    """ A simple function to compute all distances in parallel for K-mean
        ----------------------------------------------------------------------
        Inputs:
        --------
            * ğ— = a (p, N) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
            * ğ› = an array of shape (p,N) corresponding to classes centers
            * distance = function to compute distance between two samples
                         ** ğ±_1 = sample 1
                         ** ğ±_2 = sample 2
            * enable_multi = enable or not parallel compuation
            * number_of_threads = number of parallel threads (cores of machine)

        Outputs:
        ---------
            * ğ = a (N,K) numpy array containing all needed distances for K-mean
        """

    # -----------------------------------------------------------
    # Case: Multiprocessing is enabled
    # -----------------------------------------------------------
    if enable_multi:
        (p, N) = ğ—.shape
        ğ = []  # Results container
        indexes_split = np.hstack([0, int(N / number_of_threads) * np.arange(1, number_of_threads), N])
        # Separate data in subsets to be treated in parallel
        ğ—_subsets = []
        for t in range(1, number_of_threads + 1):
            ğ—_subsets.append(ğ—[:, indexes_split[t - 1]:indexes_split[t]])
        queues = [Queue() for i in range(number_of_threads)]  # Serves to obtain result for each thread
        args = [(ğ—_subsets[i], ğ›, distance, True, queues[i]) for i in range(number_of_threads)]
        jobs = [Process(target=compute_distance_k_means, args=a) for a in args]
        # Starting parallel computation
        for j in jobs: j.start()
        # Obtaining result for each thread
        for q in queues: ğ.append(q.get())
        # Waiting for each thread to terminate
        for j in jobs: j.join()

        # Merging results
        ğ = np.vstack(ğ)

        # -----------------------------------------------------------
    # Case: Multiprocessing is not enabled
    # ----------------------------------------------------------- 
    else:
        ğ = compute_distance_k_means(ğ—, ğ›, distance)

    return ğ


def random_index_for_initialisation(K, N):
    indexes = []
    for k in range(K):
        index = np.random.randint(N)
        while index in indexes:
            index = np.random.randint(N)
        indexes.append(index)
    return indexes


def choose_center_from_indexes(ğ—, indexes):
    (p, N) = ğ—.shape
    K = len(indexes)
    ğ› = np.empty((p, K)).astype(ğ—.dtype)
    for k in range(K):
        ğ›[:, k] = ğ—[:, indexes[k]]
    return ğ›


def compute_mean_k_means(
    X_class,
    mean_function,
    enable_multi=False,
    queue=None,
    jobno=None
):
    mu = mean_function(X_class)
    if enable_multi:
        # Because we want to keep teh order of the means, we have to know which index it corresponds to
        # So we return it
        queue.put([mu, jobno])
        print('Mean of class', jobno+1, 'computed !')
    else:
        return mu

 
def wrapper_compute_all_mean_parallel(
    ğ—,
    K,
    ğ“’,
    mean_function,
    enable_multi=False
):
    """ A simple function to compute all means in parallel for K-mean
        CAUTION: number of threads = K in this case because I did not want 
                 to bother managing the data communication
        ----------------------------------------------------------------------
        Inputs:
        --------
            * ğ— = a (p, N) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
            * K = number of classes 
            * ğ“’ = an array of shape (N,) with each sample with a label in {0,..., K-1}
            * mean_function = function to compute mean
                              takes two arguments:
                              ** ğ—_class = array of shape (p, M) corresponding to 
                                           samples in class
            * enable_multi = enable or not parallel compuation
 
        Outputs:
        ---------
            * ğ› = a (p,K) numpy array containing all means of classes
        """
  
    # -----------------------------------------------------------
    # Case: Multiprocessing is enabled
    # -----------------------------------------------------------
    if enable_multi:
        p = ğ—.shape[0]
        ğ› = np.empty((p,K), dtype=complex)
        number_of_effective_threads = K
        queues = [Queue() for i in range(number_of_effective_threads)]  # Serves to obtain result for each thread
        args = [(ğ—[:, ğ“’ == i], mean_function, True, queues[i], i) for i in range(number_of_effective_threads)]
        jobs = [Process(target=compute_mean_k_means, args=a) for a in args]
        # Starting parallel computation
        for j in jobs: j.start()
        # Obtaining result for each thread
        for q in queues: tmp=q.get(); ğ›[:,tmp[1]] = tmp[0]
        # Waiting for each thread to terminate
        for j in jobs: j.join()
 
    # -----------------------------------------------------------
    # Case: Multiprocessing is not enabled
    # ----------------------------------------------------------- 
    else:
        p = ğ—.shape[0]
        ğ› = np.empty((p,K), dtype=complex)
        for k in range(K):  # Looping on all classes
            print("Computing mean of class %d/%d " % (k+1,K))
            ğ—_class = ğ—[:, ğ“’==k]
            ğ›[:,k] = compute_mean_k_means(ğ—_class, mean_function)
 
    return ğ›

def compute_objective_function(distances):
    """ Compute the value of the objective function of K-means algorithm.
        See https://en.wikipedia.org/wiki/K-means_clustering
        ----------------------------------------------------------------------
        Inputs:
        --------
            * distances = distances between points and center of classes. np array of size (N, C) where N is the number of samples and C is the number of clusters.
        Outputs:
        ---------
            * result = value of the objective function
    """
    C = np.argmin(distances, axis=1)
    result = 0
    for k in np.unique(C):
        result += np.sum(distances[C==k, k])
    return result/distances.shape[0]

def K_means_clustering_algorithm(
    X,
    K,
    distance,
    mean_function,
    init=None,
    eps=1e-2,
    iter_max=20,
    enable_multi_distance=False,
    enable_multi_mean=False,
    number_of_threads=4
):
    """ K-means algorithm in a general multivariate context with an arbitary
        distance and an arbitray way to chose clusters center:
        Objective is to obtain a partion C = {C_0,..., C_{K-1}} of the data, 
        by computing centers mu_i and assigning samples by closest distance.
        ----------------------------------------------------------------------
        Inputs:
        --------
            * X = a (p, N) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
            * K = number of classes
            * distance = function to compute distance between two samples takes two arguments:
                         ** x_1 = sample 1
                         ** x_2 = sample 2
            * mean_function = function to compute mean takes one argument:
                              ** X_class = array of shape (p, M) corresponding to 
                                           samples in class
            * init = a (N) array with one class per point (for example coming from a H-alpha decomposition). If None, centers are randomly chosen among samples.
            * iter_max = number of maximum iterations of algorithm
            * enable_multi_distance = enable or not parallel computation for distance computation
            * enable_multi_mean = enable or not parallel compuation for mean computation
            * number_of_threads = number of parallel threads (cores of machine)

        Outputs:
        ---------
            * C = an array of shape (N,) with each sample with a label in {0,..., K-1}
            * mu = an array of shape (p,K) corresponding to classes centers
            * i = number of iterations done
            * delta = covnergence criterion
    """

    (p, N) = X.shape

    # -------------------------------
    # Initialisation of center means
    # -------------------------------
    if init is None:
        indexes = random_index_for_initialisation(K, N)
        mu = choose_center_from_indexes(X, indexes)
    else:
        mu = wrapper_compute_all_mean_parallel(
            X,
            K,
            init,
            mean_function,
            enable_multi=enable_multi_mean
        )

    criterion_value = np.inf
    delta = np.inf  # Difference between previous value of criterion and new value
    i = 0  # Iteration
    C = np.empty(N)  # To store clustering results
    time_distances = 0
    time_means = 0
    
    for i in range(iter_max):
        print("K-mean algorithm iteration %d" % i)

        # -----------------------------------------
        # Computing distance
        # -----------------------------------------   
        print("Computing distances of %d samples to %d classes' means" % (N,K))
        tb = time.time()
        d = compute_all_distances_k_means(
            X,
            mu,
            distance,
            enable_multi_distance,
            number_of_threads
        )
        te = time.time()
        time_distances += te-tb

        # -----------------------------------------
        # Assigning classes
        # -----------------------------------------   
        C = np.argmin(d, axis=1)
 
        # ---------------------------------------------
        # Managing algorithm convergence
        # ---------------------------------------------
        print('############################################')
        new_criterion_value = compute_objective_function(d)
        print('K-means criterion:', round(new_criterion_value, 2))
        if criterion_value != np.inf:
            delta = np.abs(criterion_value-new_criterion_value) / criterion_value
            if delta < eps:
                print('Convergence reached:', delta)
                break
        criterion_value = new_criterion_value
        print('############################################')
        
        # -----------------------------------------
        # Computing new means using assigned samples
        # -----------------------------------------
        print("Computing means of %d classes" % K)
        tb = time.time()
        mu = wrapper_compute_all_mean_parallel(
            X,
            K,
            C,
            mean_function,
            enable_multi=enable_multi_mean
        )
        te = time.time()
        time_means += te-tb
  
        print()

    print('Total time to compute distances between samples and classes:', int(time_distances), 's.')
    print('Total time to compute new means:', int(time_means), 's.')

    return (C, mu, i + 1, delta)


# ----------------------------------------------------------------------------
# 2) Spectral algorithm in a general multivariate context with an arbitary 
#    distance
# ----------------------------------------------------------------------------
def compute_distance_one_sample(ğ—, distance, enable_multi=False, queue=None):
    # -----------------------------------------------------------
    # Definition of function to be executed in parallel (or not)
    # -----------------------------------------------------------
    (p, N) = ğ—.shape
    ğ = np.empty((N,))  # To store distance from each sample
    for n in range(0, N):  # Looping on all samples
        ğ[n] = distance(ğ—[:, 0], ğ—[:, n])
    if enable_multi:
        queue.put(ğ)
    else:
        return ğ


def compute_all_distance_matrix(
    ğ—,
    distance,
    enable_multi=False,
    number_of_threads=4,
    number_operations_min=100
):
    """ A simple function to compute all distances in parallel for spectral clustering
        ----------------------------------------------------------------------
        Inputs:
        --------
            * ğ— = a (p, N) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
            * distance = function to compute distance between two samples
                     takes three arguments:
                 ** ğ±_1 = sample 1
                 ** ğ±_2 = sample 2
            * enable_multi = enable or not parallel compuation
            * number_of_threads = number of parallel threads (cores of machine)
            * number_operations_min = number of operations done at minimum per each thread
                                (if too low, there is no interest to do parallel computing)

        Outputs:
        ---------
            * ğ“ = a (N,N) numpy array corresponding to the distance matrix
        """

    (p, N) = ğ—.shape
    ğ“ = np.zeros((N, N))
    for n in tqdm(range(0, N - 1)):

        if enable_multi and (N - n - 1) > number_operations_min * number_of_threads:
            ğ_list = []  # Results container
            indexes_split = np.hstack(
                [n + 1, n + int((N - n - 1) / number_of_threads) * np.arange(1, number_of_threads), N])
            # Separate data in subsets to be treated in parallel
            ğ—_subsets = []
            for t in range(1, number_of_threads + 1):
                ğ—_subsets.append(np.hstack([ğ—[:, n].reshape((p, 1)), ğ—[:, indexes_split[t - 1]:indexes_split[t]]]))
            queues = [Queue() for i in range(number_of_threads)]  # Serves to obtain result for each thread
            args = [(ğ—_subsets[i], distance, True, queues[i]) for i in range(number_of_threads)]
            jobs = [Process(target=compute_distance_one_sample, args=a) for a in args]
            # Starting parallel computation
            for j in jobs: j.start()
            # Obtaining result for each thread
            for q in queues: ğ_list.append(q.get())
            # Waiting for each thread to terminate
            for j in jobs: j.join()

            # Merging results
            for i in range(1, len(ğ_list)):
                ğ_list[i] = ğ_list[i][1:]
            ğ = np.hstack(ğ_list)

        else:
            ğ = compute_distance_one_sample(ğ—[:, n:], distance)

        ğ“[n, n:] = ğ
        ğ“[n:, n] = ğ

    return ğ“


